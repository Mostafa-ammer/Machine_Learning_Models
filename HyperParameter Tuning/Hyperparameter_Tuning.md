# What is the importance of Hyper-Parameter Tuning ?

 Hyperparameters are critical as they carry the responsibility for the outcome of any machine learning,
deep learning model. Our goal is to find an optimal value for the hyperparameters that minimizes a loss function to give better results.


# What is Difference between Parameters and Hyperparameters ?!

**Model Parameters: **These are the parameters in the model that must be determined using the training data set

**Hyperparameters:** These are adjustable parameters that must be tuned in order to obtain a model with optimal performance



**Hyperparameter Tuning/Optimization** : The process that involves the search of the optimal values of hyperparameters for any machine learning algorithm is called hyperparameter tuning/optimization.


# A) What is the  GridSearch ?

- Grid search picks out a grid of hyperparameter values, evaluates every one of them, and returns the best.

- This means that the parameters search is done in the entire grid of the selected data.

- An important thing to remember while performing Grid Search is that the more parameters we have, the more time and space will be taken by the parameters to           perform the search.


# B) What is RandomSearch ?

Random Search replaces the exhaustive enumeration of all combinations by selecting them randomly.
Random Search is running multiple tasks simultaneously.

![image](https://miro.medium.com/v2/resize:fit:1100/format:webp/1*fYLP4FRXFl9Fvd375G8NbQ.png)
